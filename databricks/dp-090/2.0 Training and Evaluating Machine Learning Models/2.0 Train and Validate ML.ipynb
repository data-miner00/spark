{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "45f80712-2461-4fcf-9fce-9348e7d014d5",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Training and Validating a Machine Learning Model\n",
        "\n",
        "Linear regression is the most commonly employed machine learning model since it is highly interpretable and well studied.  This is often the first pass for data scientists modeling continuous variables.  This notebook trains a multivariate regression model and interprets the results. This notebook is organized in two sections:\n",
        "\n",
        "- Exercise 1: Training a Model\n",
        "- Exercise 2: Validating a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "22fda893-2242-45e6-844b-1844109462fb",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Run the following cell to load common libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a033b72b-f6a7-411e-869a-e17d649c836f",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import numpy as np\n",
        "from pyspark.sql.types import * \n",
        "from pyspark.sql.functions import col, lit\n",
        "from pyspark.sql.functions import udf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Imported common libraries.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e77e8a28-e267-4775-92e2-57025e637213",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## Load the training data\n",
        "\n",
        "In this notebook, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets]( https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. Each row of the table represents a taxi ride that includes columns such as number of passengers, trip distance, datetime information, holiday and weather information, and the taxi fare for the trip.\n",
        "\n",
        "Run the following cell to load the table into a Spark dataframe and reivew the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e218c3ef-5a41-4d5c-873c-46934a1c9991",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = spark.sql(\"select * from nyc_taxi\")\n",
        "display(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4e257d98-a8b0-4709-ad9a-b75c615a033d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## Exercise 1: Training a Model\n",
        "\n",
        "In this section we will use the Spark's machine learning library, `MLlib` to train a `NYC Taxi Fare Predictor` machine learning model. We will train a multivariate regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information. Before we start, let's review the three main abstractions that are provided in the `MLlib`:<br><br>\n",
        "\n",
        "1. A **transformer** takes a DataFrame as an input and returns a new DataFrame with one or more columns appended to it.  \n",
        "  - Transformers implement a `.transform()` method.  \n",
        "2. An **estimator** takes a DataFrame as an input and returns a model, which itself is a transformer.\n",
        "  - Estimators implements a `.fit()` method.\n",
        "3. A **pipeline** combines together transformers and estimators to make it easier to combine multiple algorithms.\n",
        "  - Pipelines implement a `.fit()` method.\n",
        "  \n",
        "These basic building blocks form the machine learning process in Spark from featurization through model training and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d3cb471d-0f0d-4806-8dfe-5d6103d6d41a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "-sandbox\n",
        "\n",
        "### Featurization of the training data\n",
        "\n",
        "Machine learning models are only as strong as the data they see and can only work on numerical data.  **Featurization is the process of creating this input data for a model.** In this section we will build derived features and create a pipeline of featurization steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "81f39d63-5392-4a92-92f4-9fa3353a4c2c",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Run the following cell to engineer the cyclical features to represent `hour_of_day`. Also, we will drop rows with null values in the `totalAmount` column and convert the column ` isPaidTimeOff ` as integer type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6128f4f4-9378-4c6e-ae9f-ed423596d4dc",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def get_sin_cosine(value, max_value):\n",
        "  sine =  np.sin(value * (2.*np.pi/max_value))\n",
        "  cosine = np.cos(value * (2.*np.pi/max_value))\n",
        "  return (sine.tolist(), cosine.tolist())\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"sine\", DoubleType(), False),\n",
        "    StructField(\"cosine\", DoubleType(), False)\n",
        "])\n",
        "\n",
        "get_sin_cosineUDF = udf(get_sin_cosine, schema)\n",
        "\n",
        "dataset = dataset.withColumn(\"udfResult\", get_sin_cosineUDF(col(\"hour_of_day\"), lit(24))).withColumn(\"hour_sine\", col(\"udfResult.sine\")).withColumn(\"hour_cosine\", col(\"udfResult.cosine\")).drop(\"udfResult\").drop(\"hour_of_day\")\n",
        "\n",
        "dataset = dataset.filter(dataset.totalAmount.isNotNull())\n",
        "\n",
        "dataset = dataset.withColumn(\"isPaidTimeOff\", col(\"isPaidTimeOff\").cast(\"integer\"))\n",
        "\n",
        "display(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "090891ff-9817-4098-83fa-1bc00c87718c",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Run the following cell to create stages in our featurization pipeline to scale the numerical features and to encode the categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c0f3ed2d-f096-4197-9ec1-014e2833024b",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "numerical_cols = [\"passengerCount\", \"tripDistance\", \"snowDepth\", \"precipTime\", \"precipDepth\", \"temperature\", \"hour_sine\", \"hour_cosine\"]\n",
        "categorical_cols = [\"day_of_week\", \"month_num\", \"normalizeHolidayName\", \"isPaidTimeOff\"]\n",
        "label_column = \"totalAmount\"\n",
        "\n",
        "stages = []\n",
        "\n",
        "inputCols = [\"passengerCount\"]\n",
        "outputCols = [\"passengerCount\"]\n",
        "imputer = Imputer(strategy=\"median\", inputCols=inputCols, outputCols=outputCols)\n",
        "stages += [imputer]\n",
        "\n",
        "assembler = VectorAssembler().setInputCols(numerical_cols).setOutputCol('numerical_features')\n",
        "scaler = MinMaxScaler(inputCol=assembler.getOutputCol(), outputCol=\"scaled_numerical_features\")\n",
        "stages += [assembler, scaler]\n",
        "\n",
        "for categorical_col in categorical_cols:\n",
        "    # Category Indexing with StringIndexer\n",
        "    stringIndexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"_index\", handleInvalid=\"skip\")\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categorical_col + \"_classVector\"])\n",
        "    # Add stages.  These are not run here, but will run all at once later on.\n",
        "    stages += [stringIndexer, encoder]\n",
        "    \n",
        "print(\"Created stages in our featurization pipeline to scale the numerical features and to encode the categorical features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9baf9f11-ddf0-498b-b19f-dcddd670c655",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Use a `VectorAssembler` to combine all the feature columns into a single vector column named **features**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7ca21c94-723f-4e4a-8317-1e67fe8d7a85",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "assemblerInputs = [c + \"_classVector\" for c in categorical_cols] + [\"scaled_numerical_features\"]\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]\n",
        "print(\"Used a VectorAssembler to combine all the feature columns into a single vector column named features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d565f2f3-f330-45bb-aa2e-58a50202361f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "-sandbox\n",
        "**Run the stages as a Pipeline**\n",
        "\n",
        "The pipeline is itself is now an `estimator`.  Call the pipeline's `fit` method and then `transform` the original dataset. This puts the data through all of the feature transformations we described in a single call. Observe the new columns, especially column: **features**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "de51dc7a-b0cb-4077-837f-c98ec889dc4d",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "partialPipeline = Pipeline().setStages(stages)\n",
        "pipelineModel = partialPipeline.fit(dataset)\n",
        "preppedDataDF = pipelineModel.transform(dataset)\n",
        "\n",
        "display(preppedDataDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5c8ac894-e128-4386-a865-03de29d39667",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "-sandbox\n",
        "\n",
        "### Train a multivariate regression model\n",
        "\n",
        "A multivariate regression takes an arbitrary number of input features. The equation for multivariate regression looks like the following where each feature `p` has its own coefficient:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;`Y ≈ β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + ... + β<sub>p</sub>X<sub>p</sub>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "35f5a2e9-2a02-43e2-80c8-a9e861835a93",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Split the featurized training data for training and validating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "894b2c1a-9c81-45df-9288-5d183b31ae31",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "(trainingData, testData) = preppedDataDF.randomSplit([0.7, 0.3], seed=97)\n",
        "print(\"The training data is split for training and validating the model: 70-30 split.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5531ccc9-2d4e-4ace-9a7b-5903a8fec927",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Create the estimator `LinearRegression` and call its `fit` method to get back the trained ML model (`lrModel`). You can read more about [Linear Regression] from the [classification and regression] section of MLlib Programming Guide.\n",
        "\n",
        "[classification and regression]: https://spark.apache.org/docs/latest/ml-classification-regression.html\n",
        "[Linear Regression]: https://spark.apache.org/docs/3.1.1/ml-classification-regression.html#linear-regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "536675fd-6a5b-4c87-9c28-299a755a2b30",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=label_column)\n",
        "\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "print(lrModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "0e1bc587-0068-4a4a-8284-d683ec694549",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## Exercise 2: Validating a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f61db909-371d-4ab1-b41e-906b1d00be88",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "-sandbox\n",
        "\n",
        "From the trained model summary, let’s review some of the model performance metrics such as, Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R<sup>2</sup> score. We will also look at the multivariate model’s coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6dea0276-37b3-4f33-9de4-f03437b0bae6",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "summary = lrModel.summary\n",
        "print(\"RMSE score: {} \\nMAE score: {} \\nR2 score: {}\".format(summary.rootMeanSquaredError, summary.meanAbsoluteError, lrModel.summary.r2))\n",
        "print(\"\")\n",
        "print(\"β0 (intercept): {}\".format(lrModel.intercept))\n",
        "i = 0\n",
        "for coef in lrModel.coefficients:\n",
        "  i += 1\n",
        "  print(\"β{} (coefficient): {}\".format(i, coef))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a9dbd05f-b72f-4411-8c85-03d0519e14c9",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "-sandbox\n",
        "\n",
        "Evaluate the model performance using the hold-back  dataset. Observe that the RMSE and R<sup>2</sup> score on holdback dataset is slightly degraded compared to the training summary. A big disparity in performance metrics between training and hold-back dataset can be an indication of model overfitting the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "53a8569d-acad-4945-b8c9-0084e314ac57",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=label_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=label_column, predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(\"MAE on test data = %g\" % mae)\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=label_column, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = evaluator.evaluate(predictions)\n",
        "print(\"R2 on test data = %g\" % r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ecc57e50-b579-48fa-b09e-8ac2fa8b4418",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "**Compare the summary statistics between the true values and the model predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dfe7ef89-4f5b-4e39-8af0-763a9e2ceb55",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(predictions.select([\"totalAmount\",  \"prediction\"]).describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e65866b2-f4bd-4647-8335-38645a7e6ef2",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "**Visualize the plot between true values and the model predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e8907456-ae01-41e7-b0ec-579a02d66472",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "p_df = predictions.select([\"totalAmount\",  \"prediction\"]).toPandas()\n",
        "true_value = p_df.totalAmount\n",
        "predicted_value = p_df.prediction\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(true_value, predicted_value, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "p1 = max(max(predicted_value), max(true_value))\n",
        "p2 = min(min(predicted_value), min(true_value))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('True Values', fontsize=15)\n",
        "plt.ylabel('Predictions', fontsize=15)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "2.0 Train and Validate ML Model",
      "widgets": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
