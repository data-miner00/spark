{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1251d57e-27e1-4634-b36b-c42dbb5d7851",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Working with data in Azure Databricks\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- viewing available tables\n",
    "- loading table data in dataframes\n",
    "- loading file/dbfs data in dataframes\n",
    "- using spark for simple queries\n",
    "- using spark to show the data and its structure\n",
    "- using spark for complex queries\n",
    "- using Databricks' `display` for custom visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add6061e-4799-443a-ba57-9c15b29be04c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Attach notebook to your cluster\n",
    "Before executing any cells in the notebook, you need to attach it to your cluster. Make sure that the cluster is running.\n",
    "\n",
    "In the notebook's toolbar, select the drop down arrow next to Detached, and then select your cluster under Attach to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cab873eb-5f8d-4f97-957e-9116f70281af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## About Spark DataFrames\n",
    "\n",
    "Spark DataFrames are distributed collections of data, organized into rows and columns, similar to traditional SQL tables.\n",
    "\n",
    "A DataFrame can be operated on using relational transformations, through the Spark SQL API, which is available in Scala, Java, Python, and R.\n",
    "\n",
    "We will use Python in our notebook. \n",
    "\n",
    "We often refer to DataFrame variables using `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa84415-bf78-413a-ac43-a7f773881d39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading data into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a7b43f-068c-4ba6-aa7c-422375ff7f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### View available data\n",
    "\n",
    "To check the data available in our Databricks environment we can use the `%sql` magic and query our tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab4f66d-c5b0-4b17-ad7f-3491591bae76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from nyc_taxi;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85aebc48-2700-44a6-a89b-6cc7cadefa3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading data from our tables\n",
    "\n",
    "Using Spark, we can read data into dataframes. \n",
    "\n",
    "It is important to note that spark has read/write support for a widely set of formats. \n",
    "It can use\n",
    "* csv\n",
    "* json\n",
    "* parquet\n",
    "* orc\n",
    "* avro\n",
    "* hive tables\n",
    "* jdbc\n",
    "\n",
    "We can read our data from the tables (since we already imported the initial csv as Databricks tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013fa30e-ff34-4781-9cdf-224f608bcefb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM nyc_taxi\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f380fe-44ee-4ccf-9f76-9c705e4f0a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading data from the DBFS\n",
    "\n",
    "We can also read the data from the files on which the data in the table is based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ed26d9-adf8-4332-b1c0-1638f8fe6b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load('dbfs:/user/hive/warehouse/nyc_taxi/')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0039425-8d3e-4ce7-bae3-26a34c100f99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### DataFrame size\n",
    "\n",
    "Use `count` to determine how many rows of data we have in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4337e2a0-dfed-47f0-b1fc-960f65bf0b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da019f1-efa0-442b-b746-3125c05089ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### DataFrame structure\n",
    "\n",
    "To get information about the schema associated with our dataframe we can use `printSchema`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015feeca-d6c2-45e4-a133-f267e05d0c0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260d9dd7-66a2-4116-88a7-37ec1ca52470",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Querying dataframes\n",
    "\n",
    "Once that spark has the data, we can manipulate it using spark SQL API.\n",
    "\n",
    "We can easily use the spark SQL dsl to do joins, aggregations, filtering. \n",
    "We can change the data structure, add or drop columns, or change the column types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc2ffd3-771a-44c3-bb20-9fda7ef8d14c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will use the python function we've already defined to convert Celsius degrees to Fahrenheit degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6344aa6f-f5bf-496f-8c2c-c8c065e02878",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def celsiusToFahrenheit(source_temp=None):\n",
    "    return(source_temp * (9.0/5.0)) + 32.0\n",
    "  \n",
    "celsiusToFahrenheit(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051ff7dd-12a8-4279-974b-1f0b8383c386",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will adapt it as a udf (user defined function) to make it usable with Spark's dataframes API.\n",
    "\n",
    "And we will use it to enrich our source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866c4a22-1c3b-493e-a3fa-f6d850296992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "udfCelsiusToFahrenheit = udf(lambda z: celsiusToFahrenheit(z), DoubleType())\n",
    "\n",
    "display(df.filter(col('temperature').isNotNull()) \\\n",
    "  .withColumn(\"tempC\", col(\"temperature\").cast(DoubleType())) \\\n",
    "  .select(col(\"tempC\"), udfCelsiusToFahrenheit(col(\"tempC\")).alias(\"tempF\")))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9333102c-590a-4969-b8fe-0b1a6078ee9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "More complex SQL functions are available in spark: \n",
    "\n",
    "* grouping, sorting, limits, count\n",
    "* aggregations: agg, max, sum\n",
    "* windowing: partitionBy, count over, max over\n",
    "\n",
    "For example may want to add a row-number column to our source data. Window functions will help with such complex queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89354d3-8ea9-4cb3-94d8-7a7651248151",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "\n",
    "display(df.orderBy('tripDistance', ascending=False) \\\n",
    "  .withColumn('rowno', row_number().over(Window.orderBy(monotonically_increasing_id()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf469f03-457e-4168-aa40-9c5f441ccdc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Data cleaning\n",
    "\n",
    "Before using the source data, we have to validate the contents. Let's see if there are any duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe239b3a-4328-4fba-b0ff-3af50ca85869",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count() - df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213cc608-39e3-4e31-bc8d-0f5433c7abab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some columns might be missing. We check the presence of null values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d930db-31f8-48da-84b8-2fb9a9270811",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8712d072-5081-4cd5-ad64-af1f18903abf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since some of our columns seem to have such null values, we'll have to fix these rows.\n",
    "\n",
    "We could either replace null values using `fillna` or ignore such rows using `dropna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410d813a-baea-4285-b034-a27fac8c3ada",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.fillna({'passengerCount':'1'}).dropna()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e9a8aa4-c2fe-41c7-ac2c-713f88191ead",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explore Summary Statistics and Data Distribution\n",
    "Predictive modeling is based on statistics and probability, so we should take a look at the summary statistics for the columns in our data. The **describe** function returns a dataframe containing the **count**, **mean**, **standard deviation**, **minimum**, and **maximum** values for each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb64e4e-f7c2-4c63-a574-b07653cdb800",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33618a44-1edd-420c-85d2-b8c1f42c0629",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Visualizing data\n",
    "\n",
    "Azure Databricks has custom support for displaying data. \n",
    "\n",
    "The `display(..)` command has multiple capabilities:\n",
    "* Presents up to 1000 records.\n",
    "* Exporting data as CSV.\n",
    "* Rendering a multitude of different graphs.\n",
    "* Rendering geo-located data on a world map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2eec6e-1394-4b68-ba9e-b8e7dc0068ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at our data using databricks visualizations:\n",
    "* Run the cell below\n",
    "* click on the second icon underneath the executed cell and choose `Bar`\n",
    "* click on the `Plot Options` button to configure the graph\n",
    "  * drag the `tripDistance` into the `Keys` list\n",
    "  * drag the `totalAmount` into the `Values` list\n",
    "  * choose `Aggregation` as `AVG`\n",
    "  * click `Apply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46da3576-ccf4-4a93-add4-97b22a00ac7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfClean = df.select(col(\"tripDistance\"), col(\"totalAmount\")).dropna()\n",
    "\n",
    "display(dfClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26731fa5-c8bd-4959-9b20-58ac4ec83c94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that the points form a diagonal line, which indicates a strong linear relationship between the trip distance and the total amount. This linear relationship shows a correlation between these two values, which we can measure statistically. \n",
    "\n",
    "The `corr` function calculates a correlation value between -1 and 1, indicating the strength of correlation between two fields. A strong positive correlation (near 1) indicates that high values for one column are often found with high values for the other, which a strong negative correlation (near -1) indicates that low values for one column are often found with high values for the other. A correlation near 0 indicates little apparent relationship between the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4673550d-1abd-4ac6-8f9d-8b93e5bcf58c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfClean.corr('tripDistance', 'totalAmount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f87c83-c407-465b-bf15-4689119a934d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Predictive modeling is largely based on statistical relationships between fields in the data. To design a good model, you need to understand how the data points relate to one another.\n",
    "\n",
    "A common way to start exploring relationships is to create visualizations that compare two or more data values. For example, modify the Plot Options of the chart above to compare the arrival delays for each carrier:\n",
    "\n",
    "* Keys: temperature\n",
    "* Series Groupings: month_num\n",
    "* Values: snowDeprh\n",
    "* Aggregation: avg\n",
    "* Display Type: Line Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7ba532-8060-47b6-be2e-e553c178e81f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c749a4a-2d84-4b6b-b8bf-05c2e780c3c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The plot now shows the relation between the month, the snow amount and the recorded temperature."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 245428210011601,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Working with data in Azure Databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
