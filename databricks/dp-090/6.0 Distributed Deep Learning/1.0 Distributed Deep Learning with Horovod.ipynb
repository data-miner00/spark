{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c1c9a479-ea20-4f49-b3e7-5a67635122c9",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Distributed deep learning training using PyTorch with HorovodRunner for MNIST\n",
        "\n",
        "This notebook illustrates the use of HorovodRunner for distributed training using PyTorch. \n",
        "It first shows how to train a model on a single node, and then shows how to adapt the code using HorovodRunner for distributed training. \n",
        "The notebook runs on CPU and GPU clusters.\n",
        "\n",
        "## Requirements\n",
        "Databricks Runtime 7.0 ML or above.  \n",
        "HorovodRunner is designed to improve model training performance on clusters with multiple workers, but multiple workers are not required to run this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5b8c482f-e7b8-4dfe-8d75-07ab5b3b1ea2",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## Set up checkpoint location\n",
        "The next cell creates a directory for saved checkpoint models. Databricks recommends saving training data under `dbfs:/ml`, which maps to `file:/dbfs/ml` on driver and worker nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "312af54a-9184-466f-b1c7-419a10153c3d",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "PYTORCH_DIR = '/dbfs/ml/horovod_pytorch'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "82f989bb-52a9-4871-adc2-6a5dea50d69c",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## Prepare single-node code\n",
        "\n",
        "First, create single-node PyTorch code. This is modified from the [Horovod PyTorch MNIST Example](https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_mnist.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "76eb87ea-fcd1-48bc-bfaa-1682e8f1ccf9",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Define a simple convolutional network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "bc505319-ba76-4df3-9bf6-ee5f7597e491",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "05d596a6-f16b-44cd-acc1-bfb20d2bc294",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "###Configure single-node training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6d009c6d-c001-4670-9e45-fe94f1bf19ac",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Specify training parameters\n",
        "batch_size = 100\n",
        "num_epochs = 3\n",
        "momentum = 0.5\n",
        "log_interval = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9b62afcd-6564-491a-8d3d-1139e5e1729d",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def train_one_epoch(model, device, data_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(data_loader) * len(data),\n",
        "                100. * batch_idx / len(data_loader), loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d8172e7a-f671-4232-91c7-a4c5459386f8",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Create methods for saving and loading model checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1ee4f89d-352d-4fbb-9da4-b5037d5e2da7",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def save_checkpoint(log_dir, model, optimizer, epoch):\n",
        "  filepath = log_dir + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n",
        "  state = {\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "  }\n",
        "  torch.save(state, filepath)\n",
        "  \n",
        "def load_checkpoint(log_dir, epoch=num_epochs):\n",
        "  filepath = log_dir + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n",
        "  return torch.load(filepath)\n",
        "\n",
        "def create_log_dir():\n",
        "  log_dir = os.path.join(PYTORCH_DIR, str(time()), 'MNISTDemo')\n",
        "  os.makedirs(log_dir)\n",
        "  return log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b3474cb1-f66e-4209-9723-e890b86229a7",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Run single-node training with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dda0ed73-7dbb-44a5-bfc6-0afe8478d302",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from time import time\n",
        "import os\n",
        "\n",
        "single_node_log_dir = create_log_dir()\n",
        "print(\"Log directory:\", single_node_log_dir)\n",
        "\n",
        "def train(learning_rate):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  train_dataset = datasets.MNIST(\n",
        "    'data', \n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "  data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = Net().to(device)\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    train_one_epoch(model, device, data_loader, optimizer, epoch)\n",
        "    save_checkpoint(single_node_log_dir, model, optimizer, epoch)\n",
        "\n",
        "    \n",
        "def test(log_dir):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  loaded_model = Net().to(device)\n",
        "  \n",
        "  checkpoint = load_checkpoint(log_dir)\n",
        "  loaded_model.load_state_dict(checkpoint['model'])\n",
        "  loaded_model.eval()\n",
        "\n",
        "  test_dataset = datasets.MNIST(\n",
        "    'data', \n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "  data_loader = torch.utils.data.DataLoader(test_dataset)\n",
        "\n",
        "  test_loss = 0\n",
        "  for data, target in data_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = loaded_model(data)\n",
        "      test_loss += F.nll_loss(output, target)\n",
        "  \n",
        "  test_loss /= len(data_loader.dataset)\n",
        "  print(\"Average test loss: {}\".format(test_loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "834a844c-53e7-4e0b-95db-e2d37314810d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Run the `train` function you just created to train a model on the driver node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "080afc54-b5bd-4ce7-b50e-560231d88de7",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "train(learning_rate = 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5230f619-a5ff-44df-baa5-b3f1ec88015a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Load and use the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e5641d0a-e650-4ab6-b721-6f3d49ba81c7",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "test(single_node_log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f960dc9e-295f-47df-bb94-237ff2fe8428",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## Migrate to HorovodRunner\n",
        "\n",
        "HorovodRunner takes a Python method that contains deep learning training code with Horovod hooks. HorovodRunner pickles the method on the driver and distributes it to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "98621a9a-ca73-493e-b9d8-4d8b1d6e9e5a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Initialize the library\n",
        "Import statement depending on which framework you're using. \n",
        "\n",
        "hvd.init() -> sets up communication between workers and \"bookkeeping\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a3110497-edcb-400a-a35b-f5cf0f09310c",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import horovod.torch as hvd\n",
        "from sparkdl import HorovodRunner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b3fdd85b-c362-4853-a8b0-9bfa07e270ef",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "hvd_log_dir = create_log_dir()\n",
        "print(\"Log directory:\", hvd_log_dir)\n",
        "\n",
        "def train_hvd(learning_rate):\n",
        "  \n",
        "  # Initialize Horovod\n",
        "  hvd.init()  \n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  \n",
        "  if device.type == 'cuda':\n",
        "    # Pin GPU to local rank\n",
        "    torch.cuda.set_device(hvd.local_rank())\n",
        "\n",
        "  train_dataset = datasets.MNIST(\n",
        "    # Use different root directory for each worker to avoid conflicts\n",
        "    root='data-%d'% hvd.rank(),  \n",
        "    train=True, \n",
        "    download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "  )\n",
        "\n",
        "  from torch.utils.data.distributed import DistributedSampler\n",
        "  \n",
        "  # Configure the sampler so that each worker gets a distinct sample of the input dataset\n",
        "  train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
        "  # Use train_sampler to load a different sample of data on each worker\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "  model = Net().to(device)\n",
        "  \n",
        "  # The effective batch size in synchronous distributed training is scaled by the number of workers\n",
        "  # Increase learning_rate to compensate for the increased batch size\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)\n",
        "\n",
        "  # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization\n",
        "  optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
        "  \n",
        "  # Broadcast initial parameters so all workers start with the same parameters\n",
        "  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
        "    # Save checkpoints only on worker 0 to prevent conflicts between workers\n",
        "    if hvd.rank() == 0:\n",
        "      save_checkpoint(hvd_log_dir, model, optimizer, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "231b795c-7e75-44c9-aa78-e9eaf4cdbd04",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Now that you have defined a training function with Horovod,  you can use HorovodRunner to distribute the work of training the model. \n",
        "\n",
        "The HorovodRunner parameter `np` sets the number of processes. This example uses a cluster with two workers, each with a single GPU, so set `np=2`. (If you use `np=-1`, HorovodRunner trains using a single process on the driver node.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d994b687-b14b-43c0-bd28-2397a4c4a90d",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "hr = HorovodRunner(np=2) \n",
        "hr.run(train_hvd, learning_rate = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ca15511e-7c1a-4a06-bd20-e0364243bd26",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "",
              "errorTraceType": null,
              "metadata": {},
              "type": "ipynbError"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "test(hvd_log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4149a68f-2901-48ab-b809-45aadfec0183",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Under the hood, HorovodRunner takes a Python method that contains deep learning training code with Horovod hooks. HorovodRunner pickles the method on the driver and distributes it to Spark workers. A Horovod MPI job is embedded as a Spark job using the barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using `mpirun`. Each Python MPI process loads the pickled user program, deserializes it, and runs it.\n",
        "\n",
        "For more information, see [HorovodRunner API documentation](https://databricks.github.io/spark-deep-learning/#api-documentation)."
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "1.0 Distributed Deep Learning with Horovod",
      "widgets": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
